{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV1l3ZkOilGV",
        "outputId": "81964591-a69f-4158-8139-f1854b2211e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1meH3ql0iz3e",
        "outputId": "eddb396a-7a78-4f4a-98a4-00b37b475f08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSz5nlUxilGY"
      },
      "source": [
        "<h3>PREPROCESSING DATA</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9-2gDx4ilGZ"
      },
      "outputs": [],
      "source": [
        "def gen_data_and_vocab():\n",
        "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
        "        data = []\n",
        "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "            dir_path = parent_path + \"/\" + newsgroup + \"/\"\n",
        "            files = [(filename, dir_path + filename)\n",
        "                     for filename in os.listdir(dir_path) \n",
        "                     if os.path.isfile(dir_path + filename)]\n",
        "            files.sort()\n",
        "            label = group_id\n",
        "            print(\"Processsing: {}-{}\".format(group_id, newsgroup))\n",
        "            for filename, filepath in files:\n",
        "                with open(filepath) as f:\n",
        "                    text=f.read().lower()\n",
        "                    words=re.split('\\W+',text)\n",
        "                    if word_count is not None:\n",
        "                        for word in words:\n",
        "                            word_count[word] += 1\n",
        "                    content = ' '.join(words)\n",
        "                    assert len(content.splitlines())==1\n",
        "                    data.append(str(label) + \"<fff>\"\n",
        "                                + filename + \"<fff>\" + content)\n",
        "        return data\n",
        "    \n",
        "    word_count=defaultdict(int)\n",
        "    path = \"/Users/Admin/DSLab Training sessions/Session_1/data/20news-bydate/\" \n",
        "    parts = [path + dir_name + '/' for dir_name in os.listdir(path)\n",
        "             if not os.path.isfile(path + dir_name)]\n",
        "    train_path, test_path = (parts[0],parts[1]) \\\n",
        "        if 'train' in parts[0] else (parts[1],parts[0])\n",
        "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
        "    newsgroup_list.sort()\n",
        "\n",
        "    train_data = collect_data_from(\n",
        "        parent_path=train_path,\n",
        "        newsgroup_list=newsgroup_list,\n",
        "        word_count=word_count\n",
        "    )\n",
        "    vocab = [word for word, freq in\n",
        "             zip(word_count.keys(),word_count.values()) if freq > 10]\n",
        "    vocab.sort()\n",
        "    with open(\"/Users/Admin/DSLab Training sessions/Session_4/data/vocab-raw.txt\",'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "    \n",
        "    test_data = collect_data_from(\n",
        "        parent_path = test_path,\n",
        "        newsgroup_list = newsgroup_list\n",
        "    )\n",
        "\n",
        "    with open(\"/Users/Admin/DSLab Training sessions/Session_4/data/20news-train-raw.txt\",'w') as f:\n",
        "        f.write('\\n'.join(train_data))\n",
        "    with open(\"/Users/Admin/DSLab Training sessions/Session_4/data/20news-test-raw.txt\",'w') as f:\n",
        "        f.write('\\n'.join(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA96cDo4ilGa",
        "outputId": "535e4a13-01ee-448b-e7b3-8d5f6d53c0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processsing: 0-alt.atheism\n",
            "Processsing: 1-comp.graphics\n",
            "Processsing: 2-comp.os.ms-windows.misc\n",
            "Processsing: 3-comp.sys.ibm.pc.hardware\n",
            "Processsing: 4-comp.sys.mac.hardware\n",
            "Processsing: 5-comp.windows.x\n",
            "Processsing: 6-misc.forsale\n",
            "Processsing: 7-rec.autos\n",
            "Processsing: 8-rec.motorcycles\n",
            "Processsing: 9-rec.sport.baseball\n",
            "Processsing: 10-rec.sport.hockey\n",
            "Processsing: 11-sci.crypt\n",
            "Processsing: 12-sci.electronics\n",
            "Processsing: 13-sci.med\n",
            "Processsing: 14-sci.space\n",
            "Processsing: 15-soc.religion.christian\n",
            "Processsing: 16-talk.politics.guns\n",
            "Processsing: 17-talk.politics.mideast\n",
            "Processsing: 18-talk.politics.misc\n",
            "Processsing: 19-talk.religion.misc\n",
            "Processsing: 0-alt.atheism\n",
            "Processsing: 1-comp.graphics\n",
            "Processsing: 2-comp.os.ms-windows.misc\n",
            "Processsing: 3-comp.sys.ibm.pc.hardware\n",
            "Processsing: 4-comp.sys.mac.hardware\n",
            "Processsing: 5-comp.windows.x\n",
            "Processsing: 6-misc.forsale\n",
            "Processsing: 7-rec.autos\n",
            "Processsing: 8-rec.motorcycles\n",
            "Processsing: 9-rec.sport.baseball\n",
            "Processsing: 10-rec.sport.hockey\n",
            "Processsing: 11-sci.crypt\n",
            "Processsing: 12-sci.electronics\n",
            "Processsing: 13-sci.med\n",
            "Processsing: 14-sci.space\n",
            "Processsing: 15-soc.religion.christian\n",
            "Processsing: 16-talk.politics.guns\n",
            "Processsing: 17-talk.politics.mideast\n",
            "Processsing: 18-talk.politics.misc\n",
            "Processsing: 19-talk.religion.misc\n"
          ]
        }
      ],
      "source": [
        "gen_data_and_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-POUWR9ilGb"
      },
      "source": [
        "<h3>ENCODING DATA</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZYV-_SmnilGc"
      },
      "outputs": [],
      "source": [
        "MAX_DOC_LENGTH=500\n",
        "def encode_data(data_path, vocab_path):\n",
        "    with open(vocab_path) as f:\n",
        "        vocab = dict([(word, word_ID +2)\n",
        "                     for word_ID, word in enumerate(f.read().splitlines())])\n",
        "    with open(data_path) as f:\n",
        "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
        "                     for line in f.read().splitlines()]\n",
        "    encoded_data=[]\n",
        "    unknown_ID = 0\n",
        "    padding_ID = 1\n",
        "    for document in documents:\n",
        "        label, doc_id, text = document\n",
        "        words = text.split()[:MAX_DOC_LENGTH]\n",
        "        sentence_length = len(words)\n",
        "        encoded_text = []\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encoded_text.append(str(vocab[word]))\n",
        "            else:\n",
        "                encoded_text.append(str(unknown_ID))\n",
        "        if len(words) < MAX_DOC_LENGTH:\n",
        "            num_padding = MAX_DOC_LENGTH - len(words)\n",
        "            for _ in range(num_padding):\n",
        "                encoded_text.append(str(padding_ID))\n",
        "\n",
        "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' +\n",
        "                           str(sentence_length) + '<fff>' + ' '.join(encoded_text))\n",
        "    \n",
        "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "    file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dir_name + '/' + file_name, 'w') as f:\n",
        "        f.write('\\n'.join(encoded_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGFrmNF2ilGc"
      },
      "outputs": [],
      "source": [
        "train_data_path = \"/Users/Admin/DSLab Training sessions/Session_4/data/20news-train-raw.txt\"\n",
        "test_data_path = \"/Users/Admin/DSLab Training sessions/Session_4/data/20news-test-raw.txt\"\n",
        "vocab_path = \"/Users/Admin/DSLab Training sessions/Session_4/data/vocab-raw.txt\"\n",
        "encode_data(train_data_path, vocab_path)\n",
        "encode_data(test_data_path, vocab_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smaoTC0AilGd"
      },
      "source": [
        "<h3>Class RNN</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yrj33SCLilGd"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES= 20\n",
        "\n",
        "class RNN:\n",
        "    tf.reset_default_graph() \n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embedding_size,\n",
        "                 lstm_size,\n",
        "                 batch_size):\n",
        "      self._vocab_size = vocab_size\n",
        "      self._embedding_size = embedding_size\n",
        "      self._lstm_size = lstm_size\n",
        "      self._batch_size = batch_size\n",
        "\n",
        "      self._data = tf.placeholder(tf.int32, shape=[batch_size,MAX_DOC_LENGTH])\n",
        "      self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "      self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "      self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "    \n",
        "    def embedding_layer(self, indices):\n",
        "      pretrained_vectors=[]\n",
        "      pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "      np.random.seed(2018)\n",
        "      for _ in range(self._vocab_size + 1):\n",
        "        pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
        "\n",
        "      pretrained_vectors = np.array(pretrained_vectors)\n",
        "      self._embedding_matrix = tf.get_variable(\n",
        "          name='embedding',\n",
        "          shape=(self._vocab_size + 2, self._embedding_size),\n",
        "          initializer=tf.constant_initializer(pretrained_vectors)\n",
        "      )\n",
        "      return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "    def LSTM_layer(self,embeddings):\n",
        "      lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "      zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "      initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "      lstm_inputs = tf.unstack(\n",
        "          tf.transpose(embeddings, perm=[1, 0, 2])\n",
        "      )\n",
        "      lstm_outputs, last_state=tf.nn.static_rnn(\n",
        "          cell=lstm_cell,\n",
        "          inputs=lstm_inputs,\n",
        "          initial_state=initial_state,\n",
        "          sequence_length=self._sentence_lengths\n",
        "      )\n",
        "      lstm_outputs = tf.unstack(\n",
        "          tf.transpose(lstm_outputs, perm=[1, 0, 2])\n",
        "      )\n",
        "      lstm_outputs = tf.concat(\n",
        "          lstm_outputs,\n",
        "          axis=0\n",
        "      )\n",
        "      mask = tf.sequence_mask(\n",
        "          lengths=self._sentence_lengths,\n",
        "          maxlen = MAX_DOC_LENGTH,\n",
        "          dtype=tf.float32\n",
        "      )\n",
        "      mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "      mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "      lstm_outputs = mask * lstm_outputs\n",
        "      lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "      lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)\n",
        "      lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
        "          tf.cast(self._sentence_lengths, tf.float32),\n",
        "          -1\n",
        "      )\n",
        "      return lstm_outputs_average\n",
        "    \n",
        "    def build_graph(self):\n",
        "      embeddings = self.embedding_layer(self._data)\n",
        "      lstm_outputs = self.LSTM_layer(embeddings)\n",
        "      weights = tf.get_variable(\n",
        "          name='final_layer_weights',\n",
        "          shape = (self._lstm_size, NUM_CLASSES),\n",
        "          initializer=tf.random_normal_initializer(seed=2023)\n",
        "      )\n",
        "      biases = tf.get_variable(\n",
        "          name='final_layer_biases',\n",
        "          shape=(NUM_CLASSES),\n",
        "          initializer=tf.random_normal_initializer(seed=2023)\n",
        "      )\n",
        "\n",
        "      logits = tf.matmul(lstm_outputs,weights) + biases\n",
        "      labels_one_hot = tf.one_hot(\n",
        "          indices = self._labels,\n",
        "          depth = NUM_CLASSES,\n",
        "          dtype = tf.float32\n",
        "      )\n",
        "\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "          labels=labels_one_hot,\n",
        "          logits = logits\n",
        "      )\n",
        "      loss = tf.reduce_mean(loss)\n",
        "      probs = tf.nn.softmax(logits)\n",
        "      predicted_labels=tf.argmax(probs, axis=1)\n",
        "      predicted_labels = tf.squeeze(predicted_labels)\n",
        "      return predicted_labels, loss\n",
        "    def trainer(self, loss, learning_rate):\n",
        "      train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "      return train_op"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaF4kBtrghEx"
      },
      "source": [
        "<h3>Data Reader</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N846aIsETs4u"
      },
      "outputs": [],
      "source": [
        "class DataReader():\n",
        "  def __init__(self, data_path, batch_size):\n",
        "    self._batch_size=batch_size\n",
        "    with open(data_path) as f:\n",
        "      d_lines = f.read().splitlines()\n",
        "    \n",
        "    self._data = []\n",
        "    self._labels = []\n",
        "    self._sentence_lengths = []\n",
        "    self._final_tokens = []\n",
        "    for data_id, line in enumerate(d_lines):\n",
        "      feature = line.split('<fff>')\n",
        "      label, doc_id, sentence_length=int(feature[0]), int(feature[1]), int(feature[2])\n",
        "      tokens = feature[3].split()\n",
        "\n",
        "      self._data.append(tokens)\n",
        "      self._labels.append(label)\n",
        "      self._sentence_lengths.append(sentence_length)\n",
        "      self._final_tokens.append(tokens[-1])\n",
        "    \n",
        "    self._data = np.array(self._data)\n",
        "    self._labels = np.array(self._labels)\n",
        "    self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "    self._final_tokens = np.array(self._final_tokens)\n",
        "\n",
        "    self._num_epoch=0\n",
        "    self._batch_id=0\n",
        "    self._size = len(self._data)\n",
        "  \n",
        "  def next_batch(self):\n",
        "    start = self._batch_id * self._batch_size\n",
        "    end = start + self._batch_size\n",
        "    self._batch_id+=1\n",
        "    if end + self._batch_size > len(self._data):\n",
        "      end = len(self._data)\n",
        "      self._num_epoch+=1\n",
        "      self._batch_id = 0\n",
        "      indices = range(len(self._data))\n",
        "      random.seed(2023)\n",
        "      random.shuffle(indices)\n",
        "      self._data, self._labels, self._sentence_lengths, self._final_tokens = self._data[indices], self._labels[indices], self._sentence_lengths[indices], self._final_tokens[indices]\n",
        "    return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAkL_ZhFgzDj"
      },
      "source": [
        "<h3>Training</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wmnirJXlg2ao"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_RNN():\n",
        "  with open(\"/content/drive/MyDrive/Data /vocab-raw.txt\",errors='ignore') as f:\n",
        "    vocab_size=len(f.read().splitlines())\n",
        "  \n",
        "  tf.set_random_seed(2023)\n",
        "  rnn = RNN(\n",
        "      vocab_size=vocab_size,\n",
        "      embedding_size=300,\n",
        "      lstm_size=50,\n",
        "      batch_size=50\n",
        "  )\n",
        "  predicted_labels, loss=rnn.build_graph()\n",
        "  train_op=rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    train_data_reader=DataReader(\n",
        "        data_path=\"/content/drive/MyDrive/Data /20news-train-encoded.txt\",\n",
        "        batch_size=50\n",
        "    )\n",
        "    test_data_reader=DataReader(\n",
        "        data_path=\"/content/drive/MyDrive/Data /20news-test-encoded.txt\",\n",
        "        batch_size=50\n",
        "    )\n",
        "    step=0\n",
        "    MAX_STEP = 1000**2\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    while step<MAX_STEP:\n",
        "      next_train_batch=train_data_reader.next_batch()\n",
        "      train_data, train_labels, train_sentence_lengths, train_final_tokens=next_train_batch\n",
        "      plabels_eval, loss_eval, _ = sess.run(\n",
        "          [predicted_labels,loss,train_op],\n",
        "          feed_dict={\n",
        "              rnn._data:train_data,\n",
        "              rnn._labels:train_labels,\n",
        "              rnn._sentence_lengths:train_sentence_lengths,\n",
        "              rnn._final_tokens: train_final_tokens\n",
        "          }\n",
        "      )\n",
        "      step+=1\n",
        "      if step % 20==0:\n",
        "        print(\"loss:\", loss_eval)\n",
        "      if train_data_reader._batch_id==0:\n",
        "        num_true_preds = 0\n",
        "        while True:\n",
        "          next_test_batch=test_data_reader.next_batch()\n",
        "          test_data, test_labels, test_sentence_lengths, test_final_tokens=next_test_batch\n",
        "          test_plabels_eval=sess.run(\n",
        "              predicted_labels,\n",
        "              feed_dict={\n",
        "                  rnn._data:test_data,\n",
        "                  rnn._labels:test_labels,\n",
        "                  rnn._sentence_lengths:test_sentence_lengths,\n",
        "                  rnn._final_tokens: test_final_tokens\n",
        "              }\n",
        "          )\n",
        "          matches=np.equal(test_plabels_eval, test_labels)\n",
        "          num_true_preds += np.sum(matches.astype(float))\n",
        "          if test_data_reader._batch_id==0:\n",
        "            break\n",
        "        print('Epoch:',train_data_reader._num_epoch)\n",
        "        print('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader._data)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ypep4G0XIjj"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_RNN()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
