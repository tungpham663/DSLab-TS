{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d14ca84",
   "metadata": {},
   "source": [
    "<h2>PREPROCESSING TEXT DATA WITH TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb568ad6",
   "metadata": {},
   "source": [
    "<li>TF-IDF with a context d in corpus D:\n",
    "$$r_{d} = [tf-idf(w_{1},d,D),tf-idf(w_{2},d,D),...,tf-idf(w_{|V|},d,D)]$$\n",
    ",where $r_{d} \\in R^{|V|}$ and $V = \\{w_{i}\\}$ is the dictionary of all words in D.\n",
    "</li>\n",
    "<br>\n",
    "<li>The formula of $tf-idf(w_{i},d,D)$ is:\n",
    "$$tf-idf(w_{i},d,D) = tf(w_{i},d,D).idf(w_{i},d,D)$$\n",
    ",where: $tf(w_{i},d,D) = \\frac{f(w_{i},d)}{max\\{f(w_{j},d):w_{j} \\in V)\\}}\\\\\n",
    "        idf(w_{i},d,D) = log_{10} \\frac{|D|}{|\\{d' \\in D:w_{i} \\in d'\\}|}$\n",
    "and $f(w_{i},d)$ is the numbers of appearance of $w_{i}$ in context d.\n",
    "</li>\n",
    "<br>\n",
    "<li>Identify V:\n",
    "    <ul>With each context d in D:\n",
    "        <li>Seperate d to some words by punctuations, then collect $W_{d}$.</li>\n",
    "        <li>Delete stop words from $W_{d}$.</li>\n",
    "        <li>Stem words in $W_{d}$.</li>\n",
    "    </ul>\n",
    "    <li>Then $V = \\bigcup_{d \\in D} W_{d}$.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134efa2f",
   "metadata": {},
   "source": [
    "<h3>GATHER DATA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b30f429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7099afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_20newsgroup_data():\n",
    "    path=\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/\"\n",
    "    #get the dirpath of train set and test set\n",
    "    dirs = [path + dirname + '/' \n",
    "            for dirname in os.listdir(path) if not os.path.isfile(path+dirname)]\n",
    "    train_dir, test_dir = (dirs[0],dirs[1]) if 'train' in dirs[0]\\\n",
    "        else (dirs[1],dirs[0])\n",
    "    #get the list of newsgroup in train set and test set\n",
    "    list_newsgroup = [newgroup for newgroup in os.listdir(train_dir)]\n",
    "    list_newsgroup.sort()\n",
    "    #store stop words \n",
    "    with open(r\"C:\\Users\\Admin\\DSLab Training sessions\\Session 1\\data\\20news-bydate\\stop_words.txt\") as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    def collect_data_from(parent_dir,newsgroup_list):\n",
    "        data=list()\n",
    "        for group_id,newsgroup in enumerate(newsgroup_list):\n",
    "            label = group_id\n",
    "            dir_path = parent_dir + '/' + newsgroup +'/'\n",
    "            #list of filename and path to file\n",
    "            files = [(filename,dir_path + filename)\n",
    "                     for filename in os.listdir(dir_path) if os.path.isfile(dir_path + filename)]\n",
    "            files.sort()\n",
    "            for filename,filepath in files:\n",
    "                #open file in folders\n",
    "                with open(filepath) as f:\n",
    "                    text = f.read().lower()\n",
    "                    #stem words that not being stop word\n",
    "                    words = [stemmer.stem(word) \n",
    "                             for word in re.split('\\W+',text) if word not in stopwords]\n",
    "                    #append to data\n",
    "                    content = ' '.join(words)\n",
    "                    assert len(content.splitlines())==1\n",
    "                    data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
    "        return data\n",
    "    #collect train data, test data, full data\n",
    "    train_data = collect_data_from(parent_dir = train_dir, newsgroup_list=list_newsgroup)\n",
    "    test_data = collect_data_from(parent_dir = test_dir, newsgroup_list=list_newsgroup)\n",
    "    full_data = train_data+test_data\n",
    "    # write datas to files\n",
    "    with open(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/20news-train-processed.txt\",'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/20news-test-processed.txt\",'w') as f:\n",
    "        f.write('\\n'.join(test_data))\n",
    "    with open(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/20news-full-processed.txt\",'w') as f:\n",
    "        f.write('\\n'.join(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b30be3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data and gather data\n",
    "gather_20newsgroup_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d098302",
   "metadata": {},
   "source": [
    "<h3>PROCESSING DATA</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1f0a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocalbulary(data_path):\n",
    "    # compute idf \n",
    "    def compute_idf(df, corpus_size):\n",
    "        assert df>0\n",
    "        return np.log10(corpus_size * 1/df)\n",
    "    #read data\n",
    "    with open(data_path) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    #initialize the dictionary of numbers of doc containing word and corpus size\n",
    "    doc_count = defaultdict(int)\n",
    "    corpus_size = len(lines)\n",
    "    #update the value of doc_count\n",
    "    for line in lines:\n",
    "        feature = line.split('<fff>')\n",
    "        text=feature[-1]\n",
    "        words = list(set(text.split()))\n",
    "        for word in words:\n",
    "            doc_count[word]+=1\n",
    "    #generate list of word and relative idf\n",
    "    words_idfs = [(word,compute_idf(document_freq,corpus_size))\n",
    "        for word,document_freq in zip(doc_count.keys(),doc_count.values()) if document_freq > 10 and not word.isdigit()]\n",
    "    words_idfs.sort(key = lambda x:-x[1])\n",
    "    #write data to file\n",
    "    print(\"Vocabulary size is:{}\".format(len(words_idfs)))\n",
    "    with open(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/words_idfs.txt\",'w') as f:\n",
    "        f.write('\\n'.join([word+'<fff>'+str(idf) for word,idf in words_idfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d955421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is:14234\n"
     ]
    }
   ],
   "source": [
    "generate_vocalbulary(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/20news-full-processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da7657cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "def get_tf_idf(datapath):\n",
    "    #get pre-computed idf values\n",
    "    with open(r\"C:\\Users\\Admin\\DSLab Training sessions\\Session 1\\data\\20news-bydate\\words_idfs.txt\") as f:\n",
    "        words_idfs = [(line.split('<fff>')[0],float(line.split('<fff>')[1])) for line in f.read().splitlines()]\n",
    "        #generate index of words\n",
    "        word_IDS = dict([(word,index) for index,(word,idf) in enumerate(words_idfs)])\n",
    "        idfs = dict(words_idfs)\n",
    "    #read data\n",
    "    with open(datapath) as f:\n",
    "        documents = [(int(line.split('<fff>')[0]),\n",
    "                     int(line.split('<fff>')[1]),\n",
    "                     line.split('<fff>')[2]) for line in f.read().splitlines()]\n",
    "    data_tf_idf=[]\n",
    "    for document in documents:\n",
    "        #get label,doc_id,text\n",
    "        label,doc_id,text=document\n",
    "        #get word set\n",
    "        words = [word for word in text.split() if word in idfs]\n",
    "        word_set = list(set(words))\n",
    "        #determine the max of frequency of words\n",
    "        max_term_freq = max([words.count(word) for word in word_set])\n",
    "        #store tf-idf values of words\n",
    "        words_tfidf = []\n",
    "        sum_square = 0.0\n",
    "        for word in word_set:\n",
    "            #calculate tf_idf value of word\n",
    "            term_freq = words.count(word)\n",
    "            tf_idf_value = term_freq *1 / max_term_freq *idfs[word]\n",
    "            words_tfidf.append((word_IDS[word],tf_idf_value))\n",
    "            #calculate sum_square\n",
    "            sum_square += tf_idf_value**2\n",
    "        #normalize tf-idf\n",
    "        words_tfidf_normalized = [str(index) + ':' + str(tf_idf_value/np.sqrt(sum_square))\n",
    "                                  for index, tf_idf_value in words_tfidf]\n",
    "        #store data\n",
    "        sparse_rep = ' '.join(words_tfidf_normalized)\n",
    "        data_tf_idf.append([label,doc_id,sparse_rep])\n",
    "    return data_tf_idf\n",
    "\n",
    "data_tf_idf = get_tf_idf(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/20news-full-processed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a22719cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#write tf-idf to file\n",
    "with open(\"C:/Users/Admin/DSLab Training sessions/Session 1/data/20news-bydate/data_tf_idf.txt\",'w') as f:\n",
    "    res = []\n",
    "    for i in range(len(data_tf_idf)):\n",
    "        line = '<fff>'.join([str(data_tf_idf[i][0]),str(data_tf_idf[i][1]),data_tf_idf[i][2]])\n",
    "        res.append(line)\n",
    "    f.write('\\n'.join(res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
